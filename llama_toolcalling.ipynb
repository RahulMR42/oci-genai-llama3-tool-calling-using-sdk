{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a80add",
   "metadata": {},
   "source": [
    "Demo of llama3 tool selections with OCI Generativie AI Inference  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "COMPARTMENT_ID = \"COMPARTMENT OCID\"\n",
    "AUTH_TYPE = \"API_KEY\" # The authentication type to use, e.g., API_KEY (default), SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL.\n",
    "CONFIG_PROFILE = \"OCI CONFIG PROFILE NAME\"\n",
    "MODEL_ID=\"meta.llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40830450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define controller\n",
    "import oci\n",
    "config = oci.config.from_file(profile_name=CONFIG_PROFILE)\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config)\n",
    "generative_ai_client = oci.generative_ai.GenerativeAiClient(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf86e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tools\n",
    "timesheet_tool = oci.generative_ai_inference.models.FunctionDefinition(\n",
    "                    type=\"FUNCTION\",\n",
    "                    name=\"timesheet_tool\",\n",
    "                    description=\"tool to use for any queries relatead to timesheet\",\n",
    "                    parameters='{\"a\": \"int\"}')\n",
    "leave_tool = oci.generative_ai_inference.models.FunctionDefinition(\n",
    "                    type=\"FUNCTION\",\n",
    "                    name=\"leave_tool\",\n",
    "                    description=\"tool for any leave related queries\",\n",
    "                    parameters='{\"a\": \"int\"}')\n",
    "generic_tool = oci.generative_ai_inference.models.FunctionDefinition(\n",
    "                    type=\"FUNCTION\",\n",
    "                    name=\"generic_tool\",\n",
    "                    description=\"tool for any queries that does not use any of the other listed tools\",\n",
    "                    parameters='{\"a\": \"int\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #Optional to disable proxy\n",
    "os.environ['no_proxy'] = '*' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a1301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_inference(prompt:str=None):\n",
    "    chat_response = generative_ai_inference_client.chat(\n",
    "        chat_details=oci.generative_ai_inference.models.ChatDetails(\n",
    "            compartment_id=COMPARTMENT_ID,\n",
    "            serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(\n",
    "                serving_type=\"ON_DEMAND\",\n",
    "                model_id=MODEL_ID),\n",
    "            chat_request=oci.generative_ai_inference.models.GenericChatRequest(\n",
    "                api_format=\"GENERIC\",\n",
    "                messages=[\n",
    "                    oci.generative_ai_inference.models.UserMessage(\n",
    "                        role=\"SYSTEM\",\n",
    "                        content=[\n",
    "                            oci.generative_ai_inference.models.TextContent(\n",
    "                                type=\"TEXT\",\n",
    "                                text=\"Be a helpful agent\")],),\n",
    "                    oci.generative_ai_inference.models.UserMessage(\n",
    "                        role=\"USER\",\n",
    "                        content=[\n",
    "                            oci.generative_ai_inference.models.TextContent(\n",
    "                                type=\"TEXT\",\n",
    "                                text=prompt),\n",
    "                                    ],\n",
    "                        name=\"user_message\")],\n",
    "                is_stream=False,\n",
    "                num_generations=1,\n",
    "                is_echo=True,\n",
    "                top_k=-1,\n",
    "                top_p=0.22750199,\n",
    "                temperature=0,\n",
    "                frequency_penalty=1.7431623,\n",
    "                presence_penalty=1.7700758,\n",
    "                max_tokens=423,\n",
    "                tool_choice=oci.generative_ai_inference.models.ToolChoiceRequired(\n",
    "                    type=\"AUTO\"),\n",
    "                tools=[timesheet_tool,leave_tool,generic_tool])),)\n",
    "    return chat_response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2681f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"how many leaves i have left with ?\",\n",
    "    \"how many leaves i can enter via timesheet\",\n",
    "    \"how many employees are in this org?\",\n",
    "    \"how many timesheet entriees i can make while i am on sebatical leave?\",\n",
    "    \"what is the minimum tenure requirement for internal job switch\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb9035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Prompt : how many leaves i have left with ?,Tool: leave_tool\n",
      "✅️ Prompt : how many leaves i can enter via timesheet,Tool: timesheet_tool\n",
      "✅️ Prompt : how many employees are in this org?,Tool: generic_tool\n",
      "✅️ Prompt : how many timesheet entriees i can make while i am on sebatical leave?,Tool: timesheet_tool\n",
      "✅️ Prompt : what is the minimum tenure requirement for internal job switch,Tool: generic_tool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for prompt in prompts:   \n",
    "    response = call_inference(prompt=prompt)\n",
    "    if response.status == 200:\n",
    "        print(f\"✅️ Prompt : {prompt},Tool: {response.data.chat_response.choices[0].message.tool_calls[0].name}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to run - {response.status}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
